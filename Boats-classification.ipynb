{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boats-Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wGYFL0pvD7-d",
        "colab_type": "code",
        "outputId": "8f2cc074-c222-41bc-97e7-bb2487ae3915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data\n",
        "!git clone https://github.com/elsheikh21/boats-classification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'boats-classification'...\n",
            "remote: Enumerating objects: 6788, done.\u001b[K\n",
            "remote: Counting objects: 100% (6788/6788), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6780/6780), done.\u001b[K\n",
            "remote: Total 6788 (delta 5), reused 6784 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (6788/6788), 333.85 MiB | 35.34 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Checking out files: 100% (6690/6690), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W4ua18PxL2Rc",
        "colab_type": "code",
        "outputId": "75aa32d3-be7a-454c-bd98-d692d7af2569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zHrqwOrUhG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1: set our directory variables\n",
        "\n",
        "- **training_dir**: where the training data will be found in set of subfolders\n",
        "<br>\n",
        "- **testing_dir**: where the testing data will be found\n",
        "<br>\n",
        "- **ground_truth_path**: file to compare against our model predicition\n"
      ]
    },
    {
      "metadata": {
        "id": "Y4lEYRkhTOk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_dir = os.path.join(os.getcwd(), 'boats-classification', 'training_set')\n",
        "testing_dir = os.path.join(os.getcwd(), 'boats-classification', 'testing_set', 'test')\n",
        "\n",
        "ground_truth_dir = os.path.join(testing_dir, 'ground_truth.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4f8oEYKLVb2L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step2: Data preprocessing & augmentation\n",
        "\n",
        "- to make sure that the model doesn't encounter the same image twice, this helps avoid overfitting, as well as, generalizing better\n",
        "- **train_datagen**: Is responsible for the images augmentation\n",
        "- **train_generator** : applies the augmentations from the data gen to the training set and saves it in the path <code>/data/training/_processed</code>"
      ]
    },
    {
      "metadata": {
        "id": "75ryBUhLV7Cn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True\n",
        ")\n",
        "\n",
        "# The augmentation for testing set\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETWbPSlxaO-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step3: Prepare the data\n",
        "- Generate batches of image data (and their labels) directly from training folders\n",
        "- **batch_size**: refers to the number of training examples utilised in one iteration, for CPU powered we use {32, 64}, for GPU boosted we use {128, 256}.\n",
        "- Save the processed images to view them for analysis\n",
        "- **target_size**: images generated of width and height (128, 128)\n",
        "- **Class mode**: Categorical as we are trying to classify categories of different boats\n",
        "- Use those generators to train our models\n",
        "- Always save the model, compile once, reuse for as long as we wish\n"
      ]
    },
    {
      "metadata": {
        "id": "aS_kwLWYaJ6-",
        "colab_type": "code",
        "outputId": "c1483219-e2a9-4f4e-cf20-e42e2c91dd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The generator will read pictures found in\n",
        "# subfolers of training_data & generate batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        training_dir, batch_size=batch_size, class_mode='categorical'\n",
        ")\n",
        "train_samples = train_generator.samples\n",
        "train_num_classes = train_generator.num_classes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4717 images belonging to 19 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aJk8IXWfqJwz",
        "colab_type": "code",
        "outputId": "c41f8e85-cffd-4a87-bd7d-a0ce048bedd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Further insights about our testing data-set\n",
        "df = pd.read_csv(ground_truth_dir, delimiter=';')\n",
        "df.columns = ['filename', 'class']\n",
        "\n",
        "to_remove_list = ['Cacciapesca', 'Caorlina',\n",
        "                  'Lanciamaggioredi10mMarrone', 'Sanpierota', 'VigilidelFuoco',\n",
        "                  'SnapshotBarcaParziale', 'SnapshotBarcaMultipla',\n",
        "                  'Mototopocorto']\n",
        "\n",
        "new_df = df[~df['class'].isin(to_remove_list)]\n",
        "\n",
        "# Similar generator, for testing data\n",
        "test_generator = test_datagen.flow_from_dataframe(new_df, testing_dir)\n",
        "\n",
        "testing_samples = len(new_df.values)\n",
        "unique_testing_classes = new_df.groupby('class')['class'].nunique()\n",
        "testing_classes_num = len(unique_testing_classes.values)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1671 images belonging to 19 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PQjkeM6GqRLo",
        "colab_type": "code",
        "outputId": "96dfb93d-c834-4f98-dfec-da1986fb18b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "print('Training set: ({}, {})'.format(train_samples, train_num_classes))\n",
        "print('Testing set: ({}, {})'.format(\n",
        "    testing_samples, testing_classes_num))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (4717, 19)\n",
            "Testing set: (1671, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VcpGbl5cYjRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step4: Training Our model\n",
        "- Other than fighting the overfitting issue using data augmentation, we try **model entropic capacity**, so modulating it depends on the number of layers and the size of each layer.\n",
        "\n",
        "- **Dropout** also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation\n",
        "\n",
        "- Models used \n",
        "  - LeNet model\n",
        "    - first set of CONV => RELU => POOL\n",
        "    - second set of CONV => RELU => POOL\n",
        "    - set of FC => RELU layers\n",
        "    - softmax classifier\n",
        "  - AlexNet model\n",
        " \n",
        "- Compile the model\n",
        "- Fit generated data to the model\n",
        "- Save model weights and model itself"
      ]
    },
    {
      "metadata": {
        "id": "dQyf1XCrYi9D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lenet_model(img_shape, train_classes,\n",
        "                weights_path=None, visualize_summary=False):\n",
        "    model = Sequential()\n",
        "\n",
        "    # first set of CONV => RELU => POOL\n",
        "    model.add(Conv2D(20, (5, 5), input_shape=img_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # second set of CONV => RELU => POOL\n",
        "    model.add(Conv2D(50, (5, 5)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # set of FC => RELU layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # softmax classifier\n",
        "    model.add(Dense(train_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    # if weightsPath is specified load the weights\n",
        "    if weights_path is not None:\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if(visualize_summary):\n",
        "        print('\\nLeNet Model:')\n",
        "        print(model.summary())\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b063TQ0FrWow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_save_lenet_model(lenet_model, train_generator, save_model=True):\n",
        "    # Tracking our model\n",
        "    csv_logger1 = CSVLogger('lenet_model_training.log')\n",
        "    # Fit generated data to the model\n",
        "    print('\\nFitting the model process has begun...\\n')\n",
        "    lenet_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(train_generator.n//train_generator.batch_size),\n",
        "        epochs=50,\n",
        "        verbose=1,\n",
        "        callbacks=[csv_logger1],\n",
        "        use_multiprocessing=True,\n",
        "        workers=0\n",
        "    )\n",
        "    print('\\nLeNet Model fitting process has ended successfully.')\n",
        "    print(\"Log file is generated with the name 'lenet_model_training.log'\")\n",
        "\n",
        "    if(save_model):\n",
        "        # Save model weights\n",
        "        lenet_model.save_weights('lenet_model_weights.h5')\n",
        "        # Save model for further analysis\n",
        "        lenet_model.save(\"lenet_model.h5\")\n",
        "\n",
        "    # for better visualizing our model log\n",
        "    lenet_log_df = pd.read_csv('lenet_model_training.log', delimiter=',')\n",
        "    lenet_log_df.to_csv('lenet_model_training.csv')\n",
        "    print(\"Log file is generated with the name 'lenet_model_training.csv'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bgb-vfh8uzMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import savetxt\n",
        "\n",
        "def predict_eval_lenet_model(lenet_model, test_generator):\n",
        "    print('LeNet Model evaluation & prediction process starting...')\n",
        "    try:\n",
        "        predict = lenet_model.predict_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        scores = lenet_model.evaluate_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        savetxt('LeNet_scores.txt', scores)\n",
        "        savetxt('LeNet_predictions.txt', predict)\n",
        "        print('Evaluation and prediction scores are saved.')\n",
        "    except BaseException as error:\n",
        "        print('An exception occurred: {}'.format(error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Chu9Shb4mSLL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def alexnet_model(img_shape, n_classes, l2_reg,\n",
        "                  weights_path=None, visualize_summary=False):\n",
        "\n",
        "    # Initialize model\n",
        "    alexnet = Sequential()\n",
        "\n",
        "    # Layer 1\n",
        "    alexnet.add(Conv2D(96, (11, 11), input_shape=img_shape,\n",
        "                       padding='same', kernel_regularizer=l2(l2_reg)))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 2\n",
        "    alexnet.add(Conv2D(256, (5, 5), padding='same'))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 3\n",
        "    alexnet.add(ZeroPadding2D((1, 1)))\n",
        "    alexnet.add(Conv2D(512, (3, 3), padding='same'))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 4\n",
        "    alexnet.add(ZeroPadding2D((1, 1)))\n",
        "    alexnet.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "\n",
        "    # Layer 5\n",
        "    alexnet.add(ZeroPadding2D((1, 1)))\n",
        "    alexnet.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 6\n",
        "    alexnet.add(Flatten())\n",
        "    alexnet.add(Dense(3072))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(Dropout(0.5))\n",
        "\n",
        "    # Layer 7\n",
        "    alexnet.add(Dense(4096))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('relu'))\n",
        "    alexnet.add(Dropout(0.5))\n",
        "\n",
        "    # Layer 8\n",
        "    alexnet.add(Dense(n_classes))\n",
        "    alexnet.add(BatchNormalization())\n",
        "    alexnet.add(Activation('softmax'))\n",
        "\n",
        "    if weights_path is not None:\n",
        "        alexnet.load_weights(weights_path)\n",
        "\n",
        "    alexnet.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if(visualize_summary):\n",
        "        print('\\nAlexNet Model:')\n",
        "        print(alexnet.summary())\n",
        "\n",
        "    return alexnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsZuIQp_m8x4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def fit_save_alexnet_model(alexnet_model, train_generator, save_model=True):\n",
        "    # Tracking the models\n",
        "    csv_logger2 = CSVLogger('alexnet_model_training.log')\n",
        "\n",
        "    print('\\nFitting the model process has begun...\\n')\n",
        "    alexnet_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(train_generator.n//train_generator.batch_size),\n",
        "        epochs=50,\n",
        "        verbose=1,\n",
        "        callbacks=[csv_logger2],\n",
        "        use_multiprocessing=True,\n",
        "        workers=0\n",
        "    )\n",
        "    print('\\nAlexNet Model fitting process has ended successfully.')\n",
        "    print(\"Log file is generated with the name 'alexnet_model_training.log'\")\n",
        "\n",
        "    if(save_model):\n",
        "        # Save model weights\n",
        "        alexnet_model.save_weights('alexnet_model_weights.h5')\n",
        "        # Save model for further analysis\n",
        "        alexnet_model.save(\"alexnet_model.h5\")\n",
        "\n",
        "    alexnet_log_df = pd.read_csv('alexnet_model_training.log', delimiter=',')\n",
        "    alexnet_log_df.to_csv('alexnet_model_training.csv')\n",
        "    print(\"Log file is generated with the name 'alexnet_model_training.csv'\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cZwC2Kzau2Ab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_eval_alexnet_model(alexnet_model, test_generator):\n",
        "    print('\\nAlexNet Model evaluation & prediction process starting...')\n",
        "    try:\n",
        "        predict = alexnet_model.predict_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        scores = alexnet_model.evaluate_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        savetxt('AlexNet_scores.txt', scores)\n",
        "        savetxt('AlexNet_predictions.txt', predict)\n",
        "        print('Evaluation and prediction scores are saved.')\n",
        "    except BaseException as error:\n",
        "        print('An exception occurred: {}'.format(error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sikCQHwFMck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "def vgg_16(img_size, weights_path=None, visualize_summary=True):\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1, 1), input_shape=img_size))\n",
        "    model.add(Conv2D(64, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(64, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(128, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(128, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1000, activation='softmax'))\n",
        "\n",
        "    if weights_path:\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(\n",
        "        optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    if(visualize_summary):\n",
        "        print('\\nVGG16 Model:')\n",
        "        print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def fit_save_vgg_16_model(vgg16_model, train_generator, save_model=True):\n",
        "    # Tracking our model\n",
        "    csv_logger = CSVLogger('VGG16_model_training.log')\n",
        "    # Fit generated data to the model\n",
        "    print('\\nFitting the model process has begun...\\n')\n",
        "    vgg16_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(train_generator.n//train_generator.batch_size),\n",
        "        epochs=50,\n",
        "        verbose=1,\n",
        "        callbacks=[csv_logger],\n",
        "        use_multiprocessing=True,\n",
        "        workers=0\n",
        "    )\n",
        "\n",
        "    if(save_model):\n",
        "        vgg16_model.save('VGG16_model.h5')\n",
        "        vgg16_model.save_weights('VGG16_model_weights.h5')\n",
        "    \n",
        "    # for better visualizing our model log\n",
        "    vgg16_log_df = pd.read_csv('VGG16_model_training.log', delimiter=',')\n",
        "    vgg16_log_df.to_csv('VGG16_model_training.csv')\n",
        "    print(\"Log file is generated with the name 'VGG16_model_training.csv'\")\n",
        "\n",
        "\n",
        "def predict_eval_vgg_16_model(vgg16_model, test_generator):\n",
        "    print('VGG16 Model evaluation & prediction process starting...')\n",
        "    try:\n",
        "        predict = lenet_model.predict_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        scores = lenet_model.evaluate_generator(\n",
        "            test_generator, test_generator.n // test_generator.batch_size,\n",
        "            verbose=1)\n",
        "        savetxt('VGG16_scores.txt', scores)\n",
        "        savetxt('VGG16_predictions.txt', predict)\n",
        "        print('Evaluation and prediction scores are saved.')\n",
        "    except BaseException as error:\n",
        "        print('An exception occurred: {}'.format(error))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NxTg29Qu86A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2463
        },
        "outputId": "4d48fb67-1eb7-4589-8e05-01b50e3676d0"
      },
      "cell_type": "code",
      "source": [
        "# LeNet Model\n",
        "lenet_model = lenet_model(img_shape=(256, 256, 3),\n",
        "                          train_classes=train_num_classes,\n",
        "                          weights_path=None, visualize_summary=True)\n",
        "\n",
        "fit_save_lenet_model(lenet_model, train_generator, save_model=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LeNet Model:\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 252, 252, 20)      1520      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 252, 252, 20)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 20)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 122, 122, 50)      25050     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 122, 122, 50)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 61, 61, 50)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 186050)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               93025500  \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 19)                9519      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 19)                0         \n",
            "=================================================================\n",
            "Total params: 93,061,589\n",
            "Trainable params: 93,061,589\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Fitting the model process has begun...\n",
            "\n",
            "Epoch 1/50\n",
            "36/36 [==============================] - 104s 3s/step - loss: 3.3534 - acc: 0.3848\n",
            "Epoch 2/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 1.3289 - acc: 0.6137\n",
            "Epoch 3/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 1.0401 - acc: 0.6968\n",
            "Epoch 4/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.8854 - acc: 0.7413\n",
            "Epoch 5/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.8013 - acc: 0.7607\n",
            "Epoch 6/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.7100 - acc: 0.7839\n",
            "Epoch 7/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.6707 - acc: 0.7968\n",
            "Epoch 8/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.6017 - acc: 0.8126\n",
            "Epoch 9/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.5548 - acc: 0.8278\n",
            "Epoch 10/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.5290 - acc: 0.8345\n",
            "Epoch 11/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.5360 - acc: 0.8299\n",
            "Epoch 12/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.4480 - acc: 0.8624\n",
            "Epoch 13/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.4555 - acc: 0.8619\n",
            "Epoch 14/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.4143 - acc: 0.8713\n",
            "Epoch 15/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.4006 - acc: 0.8669\n",
            "Epoch 16/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.3659 - acc: 0.8820\n",
            "Epoch 17/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.3705 - acc: 0.8801\n",
            "Epoch 18/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.3437 - acc: 0.8942\n",
            "Epoch 19/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.3187 - acc: 0.9031\n",
            "Epoch 20/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2930 - acc: 0.9046\n",
            "Epoch 21/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2864 - acc: 0.9085\n",
            "Epoch 22/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2766 - acc: 0.9100\n",
            "Epoch 23/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2784 - acc: 0.9102\n",
            "Epoch 24/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2505 - acc: 0.9199\n",
            "Epoch 25/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2224 - acc: 0.9293\n",
            "Epoch 26/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2385 - acc: 0.9220\n",
            "Epoch 27/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2037 - acc: 0.9336\n",
            "Epoch 28/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.2126 - acc: 0.9308\n",
            "Epoch 29/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1778 - acc: 0.9438\n",
            "Epoch 30/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1803 - acc: 0.9421\n",
            "Epoch 31/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1747 - acc: 0.9427\n",
            "Epoch 32/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1574 - acc: 0.9515\n",
            "Epoch 33/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1579 - acc: 0.9492\n",
            "Epoch 34/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1470 - acc: 0.9538\n",
            "Epoch 35/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1186 - acc: 0.9621\n",
            "Epoch 36/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1300 - acc: 0.9598\n",
            "Epoch 37/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1155 - acc: 0.9621\n",
            "Epoch 38/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1039 - acc: 0.9677\n",
            "Epoch 39/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1132 - acc: 0.9623\n",
            "Epoch 40/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1311 - acc: 0.9597\n",
            "Epoch 41/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1068 - acc: 0.9660\n",
            "Epoch 42/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1042 - acc: 0.9672\n",
            "Epoch 43/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1134 - acc: 0.9667\n",
            "Epoch 44/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.0863 - acc: 0.9700\n",
            "Epoch 45/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.0858 - acc: 0.9730\n",
            "Epoch 46/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1093 - acc: 0.9649\n",
            "Epoch 47/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.1036 - acc: 0.9674\n",
            "Epoch 48/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.0926 - acc: 0.9706\n",
            "Epoch 49/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.0751 - acc: 0.9783\n",
            "Epoch 50/50\n",
            "36/36 [==============================] - 97s 3s/step - loss: 0.0730 - acc: 0.9784\n",
            "\n",
            "LeNet Model fitting process has ended successfully.\n",
            "Log file is generated with the name 'le_model_training.log'\n",
            "Log file is generated with the name 'lenet_model_training.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b87LnGOKvBuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# AlexNet Model\n",
        "alexnet_model = alexnet_model(img_shape=(256, 256, 3),\n",
        "                              n_classes=train_num_classes, l2_reg=0.0,\n",
        "                              weights_path=None, visualize_summary=True)\n",
        "\n",
        "fit_save_alexnet_model(alexnet_model, train_generator, save_model=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "At73oDLwFOO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3627
        },
        "outputId": "39319e63-e519-4bf5-e619-79ccd97c9f74"
      },
      "cell_type": "code",
      "source": [
        "vgg16_model = vgg_16(img_size=(256, 256, 3),\n",
        "                          weights_path=None, visualize_summary=True)\n",
        "\n",
        "fit_save_vgg_16_model(lenet_model, train_generator, save_model=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "VGG16 Model:\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d_17 (ZeroPaddi (None, 258, 258, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_18 (ZeroPaddi (None, 258, 258, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_19 (ZeroPaddi (None, 130, 130, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_20 (ZeroPaddi (None, 130, 130, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_21 (ZeroPaddi (None, 66, 66, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_22 (ZeroPaddi (None, 66, 66, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_23 (ZeroPaddi (None, 66, 66, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_24 (ZeroPaddi (None, 34, 34, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_25 (ZeroPaddi (None, 34, 34, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_26 (ZeroPaddi (None, 34, 34, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_27 (ZeroPaddi (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_28 (ZeroPaddi (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_29 (ZeroPaddi (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4096)              134221824 \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 169,814,824\n",
            "Trainable params: 169,814,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Fitting the model process has begun...\n",
            "\n",
            "Epoch 1/50\n",
            "36/36 [==============================] - 101s 3s/step - loss: 0.0874 - acc: 0.9727\n",
            "Epoch 2/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0861 - acc: 0.9747\n",
            "Epoch 3/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0905 - acc: 0.9737\n",
            "Epoch 4/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.1015 - acc: 0.9673\n",
            "Epoch 5/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0655 - acc: 0.9815\n",
            "Epoch 6/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0593 - acc: 0.9823\n",
            "Epoch 7/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0766 - acc: 0.9743\n",
            "Epoch 8/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0567 - acc: 0.9805\n",
            "Epoch 9/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0627 - acc: 0.9791\n",
            "Epoch 10/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0529 - acc: 0.9834\n",
            "Epoch 11/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0726 - acc: 0.9767\n",
            "Epoch 12/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0731 - acc: 0.9785\n",
            "Epoch 13/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0535 - acc: 0.9831\n",
            "Epoch 14/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0580 - acc: 0.9845\n",
            "Epoch 15/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0517 - acc: 0.9826\n",
            "Epoch 16/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0604 - acc: 0.9817\n",
            "Epoch 17/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0475 - acc: 0.9850\n",
            "Epoch 18/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0515 - acc: 0.9821\n",
            "Epoch 19/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0893 - acc: 0.9687\n",
            "Epoch 20/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0682 - acc: 0.9775\n",
            "Epoch 21/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0466 - acc: 0.9851\n",
            "Epoch 22/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0557 - acc: 0.9820\n",
            "Epoch 23/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0501 - acc: 0.9837\n",
            "Epoch 24/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0457 - acc: 0.9852\n",
            "Epoch 25/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0405 - acc: 0.9856\n",
            "Epoch 26/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0582 - acc: 0.9811\n",
            "Epoch 27/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0529 - acc: 0.9832\n",
            "Epoch 28/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0388 - acc: 0.9869\n",
            "Epoch 29/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0703 - acc: 0.9796\n",
            "Epoch 30/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0444 - acc: 0.9884\n",
            "Epoch 31/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0453 - acc: 0.9845\n",
            "Epoch 32/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0485 - acc: 0.9853\n",
            "Epoch 33/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0373 - acc: 0.9882\n",
            "Epoch 34/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0389 - acc: 0.9874\n",
            "Epoch 35/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0446 - acc: 0.9868\n",
            "Epoch 36/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0408 - acc: 0.9889\n",
            "Epoch 37/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0459 - acc: 0.9840\n",
            "Epoch 38/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0344 - acc: 0.9876\n",
            "Epoch 39/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0377 - acc: 0.9873\n",
            "Epoch 40/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0344 - acc: 0.9905\n",
            "Epoch 41/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0292 - acc: 0.9905\n",
            "Epoch 42/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0371 - acc: 0.9893\n",
            "Epoch 43/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0282 - acc: 0.9906\n",
            "Epoch 44/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0296 - acc: 0.9904\n",
            "Epoch 45/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0450 - acc: 0.9850\n",
            "Epoch 46/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0374 - acc: 0.9884\n",
            "Epoch 47/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0350 - acc: 0.9876\n",
            "Epoch 48/50\n",
            "36/36 [==============================] - 100s 3s/step - loss: 0.0310 - acc: 0.9893\n",
            "Epoch 49/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0289 - acc: 0.9908\n",
            "Epoch 50/50\n",
            "36/36 [==============================] - 99s 3s/step - loss: 0.0296 - acc: 0.9911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RfHqMkEqd0QC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step5: Test our models (LeNet, AlexNet, VGG16)"
      ]
    },
    {
      "metadata": {
        "id": "Exi3-01Sd4iT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3d642b63-49bb-4f17-b0a8-220cc5643b01"
      },
      "cell_type": "code",
      "source": [
        "test_lenet_model = predict_eval_lenet_model(lenet_model, test_generator)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet Model evaluation & prediction process starting...\n",
            "52/52 [==============================] - 9s 172ms/step\n",
            "52/52 [==============================] - 9s 172ms/step\n",
            "Evaluation and prediction scores are saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6tdXpBD63HR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_alexnet_model = predict_eval_alexnet_model(test_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXRHGyWeJyt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "31bbdfe1-9e70-46e8-e51a-c33b30189534"
      },
      "cell_type": "code",
      "source": [
        "test_vgg16_model = predict_eval_vgg_16_model(vgg16_model, test_generator)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16 Model evaluation & prediction process starting...\n",
            "52/52 [==============================] - 9s 165ms/step\n",
            "52/52 [==============================] - 9s 173ms/step\n",
            "Evaluation and prediction scores are saved.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}